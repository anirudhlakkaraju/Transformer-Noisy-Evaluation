{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15794,"status":"ok","timestamp":1684187642275,"user":{"displayName":"PRADHA JD","userId":"04233380069318055809"},"user_tz":240},"id":"h-t39GWCNkXO","outputId":"94f08177-7451-411e-ef40-8d150b664142"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount(\"/content/gdrive\")"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44957,"status":"ok","timestamp":1684187687227,"user":{"displayName":"PRADHA JD","userId":"04233380069318055809"},"user_tz":240},"id":"KbhyWqs0kAOq","outputId":"91ef82c5-f664-405a-c83f-81890388cd4c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting nlpaug\n","  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.22.4)\n","Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.5.3)\n","Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.27.1)\n","Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (4.6.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.12.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.65.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.11.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2022.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.4)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.4.1)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n","Installing collected packages: nlpaug\n","Successfully installed nlpaug-1.1.11\n"]}],"source":["!pip install transformers\n","!pip install -U -q PyDrive\n","!pip install sentencepiece\n","!pip install nlpaug"]},{"cell_type":"markdown","source":["###Imports"],"metadata":{"id":"-b0Kh7PFMO9q"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"WFyllB4kRQct","executionInfo":{"status":"ok","timestamp":1684187704826,"user_tz":240,"elapsed":17605,"user":{"displayName":"PRADHA JD","userId":"04233380069318055809"}}},"outputs":[],"source":["# Import required libraries\n","import torch\n","import pandas as pd\n","import numpy as np\n","import re\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report, f1_score\n","import random\n","from tqdm.auto import tqdm\n","import nlpaug.augmenter.char as nac\n","import nlpaug.augmenter.word as naw\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1684187705184,"user":{"displayName":"PRADHA JD","userId":"04233380069318055809"},"user_tz":240},"id":"VJUs_qRSP9YO","outputId":"c6d7166b-4d90-4a2a-d550-c00c9429a9fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found device: Tesla T4, n_gpu: 1\n"]}],"source":["import torch\n","\n","# Confirm that the GPU is detected\n","\n","assert torch.cuda.is_available()\n","\n","# Get the GPU device name.\n","device_name = torch.cuda.get_device_name()\n","n_gpu = torch.cuda.device_count()\n","print(f\"Found device: {device_name}, n_gpu: {n_gpu}\")\n","device = torch.device(\"cuda\")"]},{"cell_type":"code","source":["def set_seed(seed):\n","  random.seed(seed)\n","  np.random.seed(seed)\n","  torch.manual_seed(seed)\n","  if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(seed)\n","\n","set_seed(42)"],"metadata":{"id":"a0REebhaMhbg","executionInfo":{"status":"ok","timestamp":1684187705185,"user_tz":240,"elapsed":7,"user":{"displayName":"PRADHA JD","userId":"04233380069318055809"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sTFvVgIZQjk6","outputId":"66279ae6-47f1-4e2e-c07e-e4bc7396f63a","executionInfo":{"status":"ok","timestamp":1684187747177,"user_tz":240,"elapsed":41998,"user":{"displayName":"PRADHA JD","userId":"04233380069318055809"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","success!\n","helper file downloaded! (helpers.py)\n"]}],"source":["!pip install transformers\n","!pip install -U -q PyDrive\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","# Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","print('success!')\n","\n","import os\n","import zipfile\n","\n","# Download helper functions file\n","helper_file = drive.CreateFile({'id': '16HW-z9Y1tM3gZ_vFpJAuwUDohz91Aac-'})\n","helper_file.GetContentFile('helpers.py')\n","print('helper file downloaded! (helpers.py)')\n","\n"]},{"cell_type":"markdown","source":["### Data download and preprocessing"],"metadata":{"id":"FSf542_wMTjp"}},{"cell_type":"code","source":["import re\n","import string\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","def clean_text(text):\n","    # Convert all text to lowercase\n","    text = text.lower()\n","\n","    # Remove punctuation\n","    text = text.translate(str.maketrans('', '', string.punctuation))\n","\n","    # Remove numbers\n","    text = re.sub(r'\\d+', '', text)\n","\n","    # Remove stopwords\n","    stop_words = set(stopwords.words('english'))\n","    tokens = nltk.word_tokenize(text)\n","    tokens = [token for token in tokens if token not in stop_words]\n","    text = ' '.join(tokens)\n","\n","    # Remove extra whitespaces\n","    text = re.sub(' +', ' ', text)\n","\n","    return text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hkiufbTtM3Kr","executionInfo":{"status":"ok","timestamp":1684187747647,"user_tz":240,"elapsed":479,"user":{"displayName":"PRADHA JD","userId":"04233380069318055809"}},"outputId":"8014495d-8fa2-4494-ec39-02adb6d20122"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"markdown","source":["###Noise Functions"],"metadata":{"id":"c3iFICQrhkRn"}},{"cell_type":"code","source":["# Noise funcs \n","\n","char_action = ['insert',\n","        'substitute',\n","        'delete',\n","        'swap',\n","]\n","\n","word_action = ['substitute',\n","        'delete',\n","        'swap',\n","]\n","\n","\n","def get_action(type):\n","  if type==\"char\":\n","    return random.choice(char_action)\n","  elif type==\"word\":\n","    return random.choice(word_action)\n","\n","\n","def augment_tweet(tweet, p=0.7):\n","    \"\"\"\n","    Augment a tweet with character-level and word-level noise.\n","\n","    Args:\n","        tweet (str): The original tweet.\n","        p (float): The probability of applying the char level augmentation.\n","\n","    Returns:\n","        str: The augmented tweet.\n","    \"\"\"\n","    # Define a list of character-level augmentation techniques\n","    char_augmenters = [\n","        nac.OcrAug(),\n","        nac.KeyboardAug(aug_char_p=0.2, aug_word_p=0.2, include_special_char=False),\n","        nac.RandomCharAug(action=get_action(\"char\"), aug_char_p=0.1, aug_word_p=0.1),\n","    ]\n","\n","    # Define a list of word-level augmentation techniques\n","    word_augmenters = [\n","        naw.SpellingAug(),\n","        naw.SplitAug(),\n","        naw.SynonymAug(),\n","        naw.RandomWordAug(aug_p=0.2, action=get_action(\"word\")),\n","    ]\n","\n","    # Randomly apply a character-level or word-level augmentation with probability p\n","    if random.random() < p:\n","        aug = random.choice(char_augmenters)\n","        augmented_tweet = aug.augment(tweet)\n","    else:\n","        aug = random.choice(word_augmenters)\n","        augmented_tweet = aug.augment(tweet)\n","        \n","    return augmented_tweet\n","\n","def add_noise(df, augmentation_percentage, task):\n","\n","  if task==\"sentiment_analysis\":\n","    # Sample 10% of the rows in the DataFrame\n","    augment_indices = df.sample(frac=augmentation_percentage).index\n","\n","    # Apply the augment_tweet function to each tweet in the sampled rows\n","    for index in augment_indices:\n","        tweet = df.loc[index, 'text']\n","        augmented_tweet = augment_tweet(tweet)\n","        df.loc[index, 'text'] = augmented_tweet\n","    \n","    return df\n","  \n","  elif task==\"question_answering\":\n","\n","    #noise functions for QA\n","\n","    return df"],"metadata":{"id":"HblA7Ux8hoAK","executionInfo":{"status":"ok","timestamp":1684187747648,"user_tz":240,"elapsed":7,"user":{"displayName":"PRADHA JD","userId":"04233380069318055809"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"id":"B-FypXfWQ0iN","executionInfo":{"status":"ok","timestamp":1684187771387,"user_tz":240,"elapsed":23743,"user":{"displayName":"PRADHA JD","userId":"04233380069318055809"}}},"outputs":[],"source":["from helpers import tokenize_and_format, flat_accuracy\n","\n","df = pd.read_csv('/content/gdrive/MyDrive/NLP Project/data/IMDB Dataset.csv')\n","df = df.rename(columns={'review':'text'})\n","df = df[['text', 'sentiment']]\n","\n","# How much of the dataset to use\n","data_size = 0.2\n","df = df.sample(frac=data_size, random_state=42)\n","\n","df['text'] = df['text'].apply(clean_text)\n","\n","# Convert the sentiment labels into numerical values\n","sentiment_map = {'positive': 0, 'negative': 1}\n","df['sentiment'] = df['sentiment'].replace(sentiment_map)\n","\n","# Find and delete any empty rows\n","empty_rows = df[df['text'].apply(lambda x: isinstance(x, str) and len(x.strip()) == 0)]\n","df.drop(empty_rows.index, inplace=True)"]},{"cell_type":"code","source":["#Adding noise of 20%\n","# Add word and char level noise 15%\n","df = add_noise(df, augmentation_percentage=0.2, task=\"sentiment_analysis\")\n","\n","# Randomly shuffle all rows\n","df = df.sample(frac=1).reset_index(drop=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vcDdta71huOR","executionInfo":{"status":"ok","timestamp":1684187784501,"user_tz":240,"elapsed":13120,"user":{"displayName":"PRADHA JD","userId":"04233380069318055809"}},"outputId":"5f6b9ce5-b89b-499c-c1ff-b72f47d21f26"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]}]},{"cell_type":"markdown","source":["### Train/val/test splits"],"metadata":{"id":"r3JNeLHnNl04"}},{"cell_type":"code","execution_count":11,"metadata":{"id":"sxjCvi_Ids5O","executionInfo":{"status":"ok","timestamp":1684187784502,"user_tz":240,"elapsed":10,"user":{"displayName":"PRADHA JD","userId":"04233380069318055809"}}},"outputs":[],"source":["# Separate the dataset into three subsets based on the sentiment labels\n","positive_reviews = df[df['sentiment'] == sentiment_map['positive']]\n","negative_reviews = df[df['sentiment'] == sentiment_map['negative']]\n","\n","# Shuffle each of the two subsets randomly\n","positive_reviews = positive_reviews.sample(frac=1, random_state=42)\n","negative_reviews = negative_reviews.sample(frac=1, random_state=42)\n","\n","# Divide each subset into training, validation, and test sets with a 70/20/10 ratio\n","train_pos, val_pos_test_pos = train_test_split(positive_reviews, test_size=0.3, random_state=42)\n","val_pos, test_pos = train_test_split(val_pos_test_pos, test_size=0.33, random_state=42)\n","\n","train_neg, val_neg_test_neg = train_test_split(negative_reviews, test_size=0.3, random_state=42)\n","val_neg, test_neg = train_test_split(val_neg_test_neg, test_size=0.33, random_state=42)\n","\n","# Merge the corresponding subsets from each sentiment back together to form the final training, validation, and test sets\n","train_set = pd.concat([train_pos, train_neg], ignore_index=True)\n","val_set = pd.concat([val_pos, val_neg], ignore_index=True)\n","test_set = pd.concat([test_pos, test_neg], ignore_index=True)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269,"referenced_widgets":["5c6f75c0f2a449ad8d1fb48c56cedf94","141abfb89c5c4e3cb82bfdc91ef3e856","13ec66b12e12423cbe8ce18e6bf47222","fe425fcdd46b4772ab9752ad3a0ada19","c72828df75ce46b6ba7e85b8a2a242ff","d986d3e39627486eb9f73c8c5c462e5a","eafa9c5f40d94b278ded4f7b683b76fa","a429355d96fc4ca6bc1f1e905f25db2d","336464fc39f14470bbcbe56c8c024b9b","1da7810555454c72afe6fd9d9e58c4bc","0301e2cd5bc74aadad588957a9328091","f079c0adf3be4c928e33d64abca6a9dd","3cc3acc07ac7453eab29a2269a2555db","6748471f3a0a4ca19b8860b66a08dabf","5756f121048e449a84b26404aadf80da","a41c9e85cd3d474d9446519d8f1c0bc9","846ae1a4116a4d11bfaa4787850313c2","5fa947c031364eedb93eab7ebe252808","5d60f14b2e9645739d8cae6cef473128","a11d1a3a3add41119f5ea479cb36195c","33d8acd3de2f430a8907849c16734608","5a52844919244b169eae3a18c7c4e552","27620ed028af497a88614776c63b353d","6ee09e9c9ccc4aeb8b178c439ce849cf","eb9a47f38d7e4be18c1545491de83610","9d29b5c03c2a4c5291d6bfab5c97d0c8","b69dd035531f4aaab9845854087c8678","7caf4a38960c43ac82b16af52ac52cb9","c413460ece434641b0650c6685ea318e","88f68910481c42e7a54d507fd2f68dcb","08a49a3d8bce45bc85f1a2cacc747100","7f6322f768ac4b03ad35e9e52269a900","7c678475e1d44f36b84efde15777547d"]},"id":"w59g7FkaSiLU","executionInfo":{"status":"ok","timestamp":1684187811271,"user_tz":240,"elapsed":0,"user":{"displayName":"PRADHA JD","userId":"04233380069318055809"}},"outputId":"1cbd8933-bccf-4c2e-9fd0-adf69d69c804"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5c6f75c0f2a449ad8d1fb48c56cedf94","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f079c0adf3be4c928e33d64abca6a9dd","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"27620ed028af497a88614776c63b353d","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Original:  entertaining jim belushi vehicle modern cockeyed version wonderful life michael caine plays sortof angel lets belush see life would like made big jim best good story supporting cast seems like real chemistry hamilton oscar contender good warmhearted fun\n","Token IDs: tensor([[  101, 14036,  3958, 19337, 20668,  2072,  4316,  2715, 10338, 17683,\n","          2094,  2544,  6919,  2166,  2745, 19881,  3248,  4066, 11253,  4850,\n","         11082, 19337, 20668,  2156,  2166,  2052,  2066,  2081,  2502,  3958,\n","          2190,  2204,  2466,  4637,  3459,  3849,  2066,  2613,  6370,  5226,\n","          7436, 20127,  2204,  4010, 27693,  4569,   102,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0]])\n"]}],"source":["# tokenize train, test and val individually\n","# For train\n","\n","texts = train_set.text.values\n","labels = train_set.sentiment.values\n","\n","# tokenize_and_format() is a helper function provided in helpers.py\n","input_ids, attention_masks = tokenize_and_format(texts)\n","\n","# Convert the lists into tensors.\n","input_ids_train = torch.cat(input_ids, dim=0)\n","attention_masks_train = torch.cat(attention_masks, dim=0)\n","labels_train = torch.tensor(labels)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', texts[0])\n","print('Token IDs:', input_ids[0])"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G-u5LOvBTcs5","executionInfo":{"status":"ok","timestamp":1684187812817,"user_tz":240,"elapsed":1658,"user":{"displayName":"PRADHA JD","userId":"04233380069318055809"}},"outputId":"78f314cd-cbbe-49d9-abc6-fbc3025778ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original:  good footage world war iera ships planes supplement excellent war drama set adriatic walter huston excellent commanding officer knows place place room personal feelings safety ship mission must always come first robert montgomery lieutenant yet mastered role leader must play combat makes bad decisions endangering submarine crew finally becomes real man court martialed dismissed navy robert young plays lieutenant junior grade jimmy durante cook paralleling war drama equally important wartime love triangle montgomery madge evans plays hustons daughter wife tragically injured aviator recommended\n","Token IDs: tensor([[  101,  2204,  8333,  2088,  2162, 29464,  2527,  3719,  9738, 12448,\n","          6581,  2162,  3689,  2275, 23400,  4787, 15876,  7106,  6581,  7991,\n","          2961,  4282,  2173,  2173,  2282,  3167,  5346,  3808,  2911,  3260,\n","          2442,  2467,  2272,  2034,  2728,  8482,  3812,  2664, 15682,  2535,\n","          3003,  2442,  2377,  4337,  3084,  2919,  6567,  2203, 25121,  2075,\n","          6982,  3626,  2633,  4150,  2613,  2158,  2457,  7761,  2098,  7219,\n","          3212,  2728,  2402,   102]])\n"]}],"source":["# For test\n","texts = test_set.text.values\n","labels = test_set.sentiment.values\n","\n","# tokenize_and_format() is a helper function provided in helpers.py\n","input_ids, attention_masks = tokenize_and_format(texts)\n","\n","# Convert the lists into tensors.\n","input_ids_test = torch.cat(input_ids, dim=0)\n","attention_masks_test = torch.cat(attention_masks, dim=0)\n","labels_test = torch.tensor(labels)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', texts[0])\n","print('Token IDs:', input_ids[0])"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XQDvnkEBTpV9","executionInfo":{"status":"ok","timestamp":1684187817687,"user_tz":240,"elapsed":4872,"user":{"displayName":"PRADHA JD","userId":"04233380069318055809"}},"outputId":"50049249-30b3-46a2-8c70-0fe86bc5074b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original:  john van druten broadway hit brought screen maximum star power romantic fantasy modernday witch beguiles successful manhattan publisher james stewart may get top billing kim novak steals show one alluring witches ever cast spell movie screen lead pairing fact one movies weaknesses grayhaired stewart seems bit old role easy see falls hard novak little harder understand finds attractive seem mismatched temperment outlook one storys amusing conceits witches warlocks portrayed greenwich village beatniks bohemians curiously stewartnovak pairing would generate lot heat vertigo released year film vertigo compelling suspense story benefit alfred hitchcocks directionbr br films comic moments mostly provided stellar supporting cast including young jack lemmon kims warlock brother elsa lanchester ditzy aunt ernie kovacs befuddled writer hermione gingold even shows hilarious cameo sort grand witch theres lots like moviewit romance great castthat possibly take eyes enchanting miss novak seen movie half dozen times never\n","Token IDs: tensor([[  101,  2198,  3158,  2852, 10421,  2078,  5934,  2718,  2716,  3898,\n","          4555,  2732,  2373,  6298,  5913,  2715, 10259,  6965, 11693, 19231,\n","          2229,  3144,  7128,  6674,  2508,  5954,  2089,  2131,  2327, 25640,\n","          5035, 19580, 15539,  2265,  2028,  2035, 12228, 12566,  2412,  3459,\n","          6297,  3185,  3898,  2599, 22778,  2755,  2028,  5691, 21775,  3897,\n","         26227,  2098,  5954,  3849,  2978,  2214,  2535,  3733,  2156,  4212,\n","          2524, 19580,  2210,   102]])\n"]}],"source":["# For val\n","texts = val_set.text.values\n","labels = val_set.sentiment.values\n","\n","# tokenize_and_format() is a helper function provided in helpers.py ###\n","input_ids, attention_masks = tokenize_and_format(texts)\n","\n","# Convert the lists into tensors.\n","input_ids_val = torch.cat(input_ids, dim=0)\n","attention_masks_val = torch.cat(attention_masks, dim=0)\n","labels_val = torch.tensor(labels)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', texts[0])\n","print('Token IDs:', input_ids[0])"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wyYzcjXzT7e9","executionInfo":{"status":"ok","timestamp":1684187817688,"user_tz":240,"elapsed":8,"user":{"displayName":"PRADHA JD","userId":"04233380069318055809"}},"outputId":"24bb3aeb-8e77-464d-8401-89f8e3f1c7d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train Set Size:  6999\n","Validation Set Size:  2010\n","Test Set Size:  991\n"]}],"source":["#printing out len of train,test val\n","total = len(df)\n","num_train = len(train_set)\n","num_val = len(val_set)\n","num_test = len(test_set)\n","\n","print('Train Set Size: ',num_train)\n","print('Validation Set Size: ',num_val)\n","print('Test Set Size: ',num_test)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"HDKpkqDyXwg4","executionInfo":{"status":"ok","timestamp":1684187817689,"user_tz":240,"elapsed":3,"user":{"displayName":"PRADHA JD","userId":"04233380069318055809"}}},"outputs":[],"source":["# make lists of 3-tuples \n","train_dataset=[]\n","for i in range(num_train):\n","  train_dataset.append((input_ids_train[i], attention_masks_train[i], labels_train[i]))\n","\n","val_dataset=[]\n","for i in range(num_val):\n","  val_dataset.append((input_ids_val[i], attention_masks_val[i], labels_val[i]))\n","\n","test_dataset=[]\n","for i in range(num_test):\n","  test_dataset.append((input_ids_test[i], attention_masks_test[i], labels_test[i]))\n"]},{"cell_type":"markdown","source":["### Training"],"metadata":{"id":"hpzmG9IMOEQm"}},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":188,"referenced_widgets":["1ac5bfe6c20940548d23b39cf9afdd75","0c6a615c35ad43c8b8127c696f0c6fc7","464947777df342f79fec9afdf95705b8","2d58ae7cf36d49fc9c880e4e404dd323","9622339ef64b42d3b15b3493d85d851c","00ae08cd7ebb47aa802afe4a134c0e2c","f3f94735529a42f29c76362b04272b93","b265a3ab6fe54c54b0f598b33f26c418","4ac8ab10f672499d98816a4937423ea7","31241849a28443e6a62957db0c9dfe73","59aec8f5422c46a6893e45856a25db6c"]},"id":"dSu09pqLTEZo","executionInfo":{"status":"ok","timestamp":1684187833309,"user_tz":240,"elapsed":15623,"user":{"displayName":"PRADHA JD","userId":"04233380069318055809"}},"outputId":"8db4d7a7-baf7-4c21-8a08-84f805c040b6"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ac5bfe6c20940548d23b39cf9afdd75"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","# Define the BERT model for sequence classification\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()\n","\n","# Set the optimizer and learning rate scheduler\n","optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n","total_steps = len(train_dataset) * 4\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"2OxQEBsQXfko","executionInfo":{"status":"ok","timestamp":1684102179857,"user_tz":240,"elapsed":24,"user":{"displayName":"Neha Pendem","userId":"08565841284854508226"}},"outputId":"cb66e6f6-29f1-4eca-973e-3fadcebfbdb9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'# Define the training loop\\nmodel.to(device)\\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\\nbest_val_loss = float(\"inf\")\\nnum_epochs = 2\\ntrain_loss = []\\nval_loss = []\\nfor epoch in range(num_epochs):\\n  # Train the model\\n  model.train()\\n  epoch_loss = 0\\n\\n  train_loop = tqdm(train_loader, desc=f\\'Training Epoch {epoch+1}\\', leave=True)\\n\\n  for batch in train_loop:\\n      \\n    inputs = {\\'input_ids\\': batch[0].to(device),\\n              \\'attention_mask\\': batch[1].to(device),\\n              \\'labels\\': batch[2].to(device)}\\n    optimizer.zero_grad()\\n    outputs = model(**inputs)\\n    loss = outputs[0]\\n\\n    loss.backward()\\n    optimizer.step()\\n    scheduler.step()\\n\\n    epoch_loss += loss.item()\\n    train_loop.set_postfix(loss=loss.item())\\n\\n  epoch_loss /= len(train_loader)\\n  train_loss.append(epoch_loss)\\n\\n  # Evaluate the model on the validation set\\n  model.eval()\\n  val_preds = []\\n  val_labels = []\\n  epoch_val_loss = 0\\n\\n  with torch.no_grad():\\n    for batch in val_loader:\\n        \\n      inputs = {\\'input_ids\\': batch[0].to(device),\\n                \\'attention_mask\\': batch[1].to(device),\\n                \\'labels\\': batch[2].to(device)}\\n      \\n      outputs = model(**inputs)\\n      loss = outputs[0]\\n      epoch_val_loss += loss.item()\\n\\n      logits = outputs[1]\\n      preds = torch.argmax(logits, axis=1)\\n      val_preds.extend(preds.cpu().numpy())\\n      val_labels.extend(batch[2].cpu().numpy())\\n\\n    epoch_val_loss /= len(val_loader)\\n\\n  if epoch_val_loss < best_val_loss:\\n    best_val_loss = epoch_val_loss\\n    # torch.save(model.state_dict(), \"t5_sentiment_model.pt\")\\n    path = \\'/content/gdrive/MyDrive/NLP Project/models/BERT_SA_NOISY20\\'\\n\\n    torch.save(model.state_dict(), path+\\'/model_parameters.pth\\')\\n\\n  # Compute the evaluation metrics\\n  val_accuracy = accuracy_score(val_labels, val_preds)\\n  val_report = classification_report(val_labels, val_preds, target_names=[\\'positive\\', \\'negative\\'])\\n  \\n\\n  # Print the results for the current epoch\\n  print(\\'Epoch:\\', epoch+1, \\', Training Loss:\\', epoch_loss/len(train_loader), \\', Validation Loss:\\', epoch_val_loss, \\', Validation Accuracy:\\', val_accuracy)\\n  print(\\'Validation Classification Report:\\')\\n  print(val_report)\\n    '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}],"source":["'''# Define the training loop\n","model.to(device)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n","best_val_loss = float(\"inf\")\n","num_epochs = 2\n","train_loss = []\n","val_loss = []\n","for epoch in range(num_epochs):\n","  # Train the model\n","  model.train()\n","  epoch_loss = 0\n","\n","  train_loop = tqdm(train_loader, desc=f'Training Epoch {epoch+1}', leave=True)\n","\n","  for batch in train_loop:\n","      \n","    inputs = {'input_ids': batch[0].to(device),\n","              'attention_mask': batch[1].to(device),\n","              'labels': batch[2].to(device)}\n","    optimizer.zero_grad()\n","    outputs = model(**inputs)\n","    loss = outputs[0]\n","\n","    loss.backward()\n","    optimizer.step()\n","    scheduler.step()\n","\n","    epoch_loss += loss.item()\n","    train_loop.set_postfix(loss=loss.item())\n","\n","  epoch_loss /= len(train_loader)\n","  train_loss.append(epoch_loss)\n","\n","  # Evaluate the model on the validation set\n","  model.eval()\n","  val_preds = []\n","  val_labels = []\n","  epoch_val_loss = 0\n","\n","  with torch.no_grad():\n","    for batch in val_loader:\n","        \n","      inputs = {'input_ids': batch[0].to(device),\n","                'attention_mask': batch[1].to(device),\n","                'labels': batch[2].to(device)}\n","      \n","      outputs = model(**inputs)\n","      loss = outputs[0]\n","      epoch_val_loss += loss.item()\n","\n","      logits = outputs[1]\n","      preds = torch.argmax(logits, axis=1)\n","      val_preds.extend(preds.cpu().numpy())\n","      val_labels.extend(batch[2].cpu().numpy())\n","\n","    epoch_val_loss /= len(val_loader)\n","\n","  if epoch_val_loss < best_val_loss:\n","    best_val_loss = epoch_val_loss\n","    # torch.save(model.state_dict(), \"t5_sentiment_model.pt\")\n","    path = '/content/gdrive/MyDrive/NLP Project/models/BERT_SA_NOISY20'\n","\n","    torch.save(model.state_dict(), path+'/model_parameters.pth')\n","\n","  # Compute the evaluation metrics\n","  val_accuracy = accuracy_score(val_labels, val_preds)\n","  val_report = classification_report(val_labels, val_preds, target_names=['positive', 'negative'])\n","  \n","\n","  # Print the results for the current epoch\n","  print('Epoch:', epoch+1, ', Training Loss:', epoch_loss/len(train_loader), ', Validation Loss:', epoch_val_loss, ', Validation Accuracy:', val_accuracy)\n","  print('Validation Classification Report:')\n","  print(val_report)\n","    '''"]},{"cell_type":"markdown","source":["###Evaluation"],"metadata":{"id":"_Rpie9JCvGIr"}},{"cell_type":"code","source":["from helpers import tokenize_and_format, flat_accuracy\n","\n","test_df = pd.read_csv('/content/gdrive/MyDrive/NLP Project/data/IMDB Dataset.csv')\n","test_df = test_df.rename(columns={'review':'text'})\n","test_df = test_df[['text', 'sentiment']]\n","\n","# How much of the dataset to use\n","data_size = 0.2\n","test_df = test_df.sample(frac=data_size, random_state=42)\n","\n","test_df['text'] = test_df['text'].apply(clean_text)\n","\n","# Convert the sentiment labels into numerical values\n","sentiment_map = {'positive': 0, 'negative': 1}\n","test_df['sentiment'] = test_df['sentiment'].replace(sentiment_map)\n","\n","# Find and delete any empty rows\n","empty_rows = test_df[test_df['text'].apply(lambda x: isinstance(x, str) and len(x.strip()) == 0)]\n","test_df.drop(empty_rows.index, inplace=True)\n"],"metadata":{"id":"yEp_z3bBTGi3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Separate the dataset into three subsets based on the sentiment labels\n","positive_reviews = test_df[test_df['sentiment'] == sentiment_map['positive']]\n","negative_reviews = test_df[test_df['sentiment'] == sentiment_map['negative']]\n","\n","# Shuffle each of the two subsets randomly\n","positive_reviews = positive_reviews.sample(frac=1, random_state=42)\n","negative_reviews = negative_reviews.sample(frac=1, random_state=42)\n","\n","# Divide each subset into training, validation, and test sets with a 70/20/10 ratio\n","train_pos, val_pos_test_pos = train_test_split(positive_reviews, test_size=0.3, random_state=42)\n","val_pos, test_pos = train_test_split(val_pos_test_pos, test_size=0.33, random_state=42)\n","\n","train_neg, val_neg_test_neg = train_test_split(negative_reviews, test_size=0.3, random_state=42)\n","val_neg, test_neg = train_test_split(val_neg_test_neg, test_size=0.33, random_state=42)\n","\n","# Merge the corresponding subsets from each sentiment back together to form the final training, validation, and test sets\n","train_set = pd.concat([train_pos, train_neg], ignore_index=True)\n","val_set = pd.concat([val_pos, val_neg], ignore_index=True)\n","test_set = pd.concat([test_pos, test_neg], ignore_index=True)\n","\n","# For test\n","texts = test_set.text.values\n","labels = test_set.sentiment.values\n","\n","# tokenize_and_format() is a helper function provided in helpers.py\n","input_ids, attention_masks = tokenize_and_format(texts)\n","\n","# Convert the lists into tensors.\n","input_ids_test = torch.cat(input_ids, dim=0)\n","attention_masks_test = torch.cat(attention_masks, dim=0)\n","\n","labels_test = torch.tensor(labels)\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', texts[0])\n","print('Token IDs:', input_ids[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wv9-Vl4RTKGu","executionInfo":{"status":"ok","timestamp":1684102195306,"user_tz":240,"elapsed":3157,"user":{"displayName":"Neha Pendem","userId":"08565841284854508226"}},"outputId":"4bc39ded-479d-46eb-855c-f7bf8c46f85d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original:  one movies dont require brain thinking funny time pass forgot next hour really surprised john abrahams acting usually playing gangster like character emotionless faceso playing complete opposite successfullyby managing shine amongst comic geniuses paresh rawal akshaye kumar also quite surprised akshayes girls roles dont require much talent mostly moaning akshayes dissapearenceto girls surprised managed establish actual persona could differentiate good thing also majority songs goodit colourful fun boring sunday evening sure lighten mood\n","Token IDs: tensor([[  101,  2028,  5691,  2123,  2102,  5478,  4167,  3241,  6057,  2051,\n","          3413,  9471,  2279,  3178,  2428,  4527,  2198,  8181,  2015,  3772,\n","          2788,  2652, 20067,  2066,  2839,  7603,  3238,  5344,  2080,  2652,\n","          3143,  4500,  5147,  3762,  6605, 12342,  5921,  5021, 11067,  2229,\n","         11968,  9953,  6315,  2389, 17712,  7377,  6672,  9600,  2036,  3243,\n","          4527, 17712,  7377, 23147,  3057,  4395,  2123,  2102,  5478,  2172,\n","          5848,  3262, 22653,   102]])\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZcPJWMs-ekqj","executionInfo":{"status":"ok","timestamp":1684102199743,"user_tz":240,"elapsed":4441,"user":{"displayName":"Neha Pendem","userId":"08565841284854508226"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7edc7068-6a32-4ed7-e5a2-67022e422e5f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":21}],"source":["#Load model for evaluations, comment for finetuning\n","\n","path = '/content/gdrive/MyDrive/NLP Project/models/BERT_SA_NOISY20'\n","model.load_state_dict(torch.load(path+'/model_parameters.pth'))"]},{"cell_type":"code","source":["# Move input tensors to the same device as the model\n","input_ids_test = input_ids_test.to(device)\n","attention_masks_test = attention_masks_test.to(device)\n","labels_test = labels_test.to(device)\n","\n","# Set model to evaluation mode\n","model.eval()\n","\n","# Generate predictions\n","with torch.no_grad():\n","    outputs = model(input_ids=input_ids_test, attention_mask=attention_masks_test)\n","    logits = outputs.logits\n","\n","# Apply softmax to obtain probabilities\n","probs = torch.softmax(logits, dim=1)\n","preds = torch.argmax(probs, dim=1)\n","\n","# Move predictions and labels back to CPU for evaluation\n","preds = preds.detach().cpu().numpy()\n","labels_test = labels_test.cpu().numpy()\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(labels_test, preds)\n","print(f'Test Accuracy: {accuracy:.4f}')\n","\n","# Calculate F1 score\n","f1 = f1_score(labels_test, preds, average='weighted')\n","print(f'F1 Score: {f1:.4f}')\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PlrA-KEWvL6y","executionInfo":{"status":"ok","timestamp":1684102205603,"user_tz":240,"elapsed":5879,"user":{"displayName":"Neha Pendem","userId":"08565841284854508226"}},"outputId":"c3f8d896-a683-4cdb-b59d-0d932c538ef1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.8910\n","F1 Score: 0.8909\n"]}]},{"cell_type":"markdown","source":["### Evaluation on Noisy Data"],"metadata":{"id":"DklDGsG3ZpnO"}},{"cell_type":"code","source":["import random\n","random.seed(42)\n","random_noise = random.uniform(0.05, 0.15)\n","print(random_noise)\n","\n","# Download data for Testing\n","df_test = pd.read_csv('/content/gdrive/MyDrive/NLP Project/data/IMDB Dataset.csv')\n","df_test = df_test.rename(columns={'review':'text'})\n","df_test = df_test[['text', 'sentiment']]\n","\n"," \n","# How much of the dataset to use\n","data_size = 0.2\n","df_test = df_test.sample(frac=data_size, random_state=42)\n","\n","df_test['text'] = df_test['text'].apply(clean_text)\n","\n","# Convert the sentiment labels into numerical values\n","sentiment_map = {'positive': 0, 'negative': 1}\n","df_test['sentiment'] = df_test['sentiment'].replace(sentiment_map)\n","\n","# Find and delete any empty rows\n","empty_rows = df_test[df_test['text'].apply(lambda x: isinstance(x, str) and len(x.strip()) == 0)]\n","df_test.drop(empty_rows.index, inplace=True)\n","\n","df_test = add_noise(df_test, augmentation_percentage=random_noise, task=\"sentiment_analysis\")\n","\n","# Separate the dataset into three subsets based on the sentiment labels\n","positive_reviews = df_test[df_test['sentiment'] == sentiment_map['positive']]\n","negative_reviews = df_test[df_test['sentiment'] == sentiment_map['negative']]\n","\n","# Shuffle each of the two subsets randomly\n","positive_reviews = positive_reviews.sample(frac=1, random_state=42)\n","negative_reviews = negative_reviews.sample(frac=1, random_state=42)\n","\n","print(len(positive_reviews), len(negative_reviews))\n","# Divide each subset into training, validation, and test sets with a 70/20/10 ratio\n","train_pos, val_pos_test_pos = train_test_split(positive_reviews, test_size=0.3, random_state=42)\n","val_pos, test_pos = train_test_split(val_pos_test_pos, test_size=0.33, random_state=42)\n","\n","train_neg, val_neg_test_neg = train_test_split(negative_reviews, test_size=0.3, random_state=42)\n","val_neg, test_neg = train_test_split(val_neg_test_neg, test_size=0.33, random_state=42)\n","\n","# Merge the corresponding subsets from each sentiment back together to form the final training, validation, and test sets\n","train_set = pd.concat([train_pos, train_neg], ignore_index=True)\n","val_set = pd.concat([val_pos, val_neg], ignore_index=True)\n","test_set = pd.concat([test_pos, test_neg], ignore_index=True)\n","\n","# For test\n","\n","texts = test_set.text.values\n","labels = test_set.sentiment.values\n","\n","### tokenize_and_format() is a helper function provided in helpers.py ###\n","input_ids, attention_masks = tokenize_and_format(texts)\n","\n","# Convert the lists into tensors.\n","input_ids_test = torch.cat(input_ids, dim=0)\n","attention_masks_test = torch.cat(attention_masks, dim=0)\n","labels_test = torch.tensor(labels)\n","\n","\n","test_dataset=[]\n","for i in range(num_test):\n","  test_dataset.append((input_ids_test[i], attention_masks_test[i], labels_test[i]))\n","\n","import torch\n","\n","# Specify device\n","device = torch.device('cpu')\n","\n","# Load model for evaluations, comment for finetuning\n","path = '/content/gdrive/MyDrive/NLP Project/models/BERT_SA_NOISY20'\n","model.load_state_dict(torch.load(path+'/model_parameters.pth', map_location=device))\n","\n","model.to(device)\n","input_ids_test = input_ids_test.to(device)\n","attention_masks_test = attention_masks_test.to(device)\n","labels_test = labels_test.to(device)\n","\n","# Set model to evaluation mode\n","model.eval()\n","\n","# Generate predictions\n","with torch.no_grad():\n","    outputs = model(input_ids=input_ids_test, attention_mask=attention_masks_test)\n","    logits = outputs.logits\n","\n","# Apply softmax to obtain probabilities\n","probs = torch.softmax(logits, dim=1)\n","preds = torch.argmax(probs, dim=1)\n","\n","# Move predictions and labels back to CPU for evaluation\n","preds = preds.detach().cpu().numpy()\n","labels_test = labels_test.cpu().numpy()\n","\n","# Calculate accuracy\n","accuracy = (preds == labels_test).mean()\n","print(f'Test Accuracy: {accuracy:.4f}')\n","\n","from sklearn.metrics import f1_score\n","\n","f1 = f1_score(labels_test, preds, average='weighted')\n","print(f'F1 Score: {f1:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FubLrXARZs7r","executionInfo":{"status":"ok","timestamp":1684188053940,"user_tz":240,"elapsed":220633,"user":{"displayName":"PRADHA JD","userId":"04233380069318055809"}},"outputId":"b67ee8aa-b1b0-4793-9786-262df089ca2d"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["0.11394267984578837\n","5039 4961\n","Test Accuracy: 0.8436\n","F1 Score: 0.8423\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1A6gmrjpWuvPQbUyb1zQnzADjEreS150R","timestamp":1683983381300},{"file_id":"17xldgP7qyR5i18vNSvV9ipM83uROEtAv","timestamp":1683938792566},{"file_id":"1D9h1hvZHcTByfSuiUrSQ6gD_2TI9SEaR","timestamp":1683937677304},{"file_id":"1QcOxg4NUIHZiKRFTw8CafCVMeHURYrjP","timestamp":1683414003391},{"file_id":"1m_3Zcx57629wlNaV6OXeXdgwsMAVjeDH","timestamp":1683057305184}],"gpuType":"T4"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5c6f75c0f2a449ad8d1fb48c56cedf94":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_141abfb89c5c4e3cb82bfdc91ef3e856","IPY_MODEL_13ec66b12e12423cbe8ce18e6bf47222","IPY_MODEL_fe425fcdd46b4772ab9752ad3a0ada19"],"layout":"IPY_MODEL_c72828df75ce46b6ba7e85b8a2a242ff"}},"141abfb89c5c4e3cb82bfdc91ef3e856":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d986d3e39627486eb9f73c8c5c462e5a","placeholder":"​","style":"IPY_MODEL_eafa9c5f40d94b278ded4f7b683b76fa","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"13ec66b12e12423cbe8ce18e6bf47222":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a429355d96fc4ca6bc1f1e905f25db2d","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_336464fc39f14470bbcbe56c8c024b9b","value":231508}},"fe425fcdd46b4772ab9752ad3a0ada19":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1da7810555454c72afe6fd9d9e58c4bc","placeholder":"​","style":"IPY_MODEL_0301e2cd5bc74aadad588957a9328091","value":" 232k/232k [00:00&lt;00:00, 5.06MB/s]"}},"c72828df75ce46b6ba7e85b8a2a242ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d986d3e39627486eb9f73c8c5c462e5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eafa9c5f40d94b278ded4f7b683b76fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a429355d96fc4ca6bc1f1e905f25db2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"336464fc39f14470bbcbe56c8c024b9b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1da7810555454c72afe6fd9d9e58c4bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0301e2cd5bc74aadad588957a9328091":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f079c0adf3be4c928e33d64abca6a9dd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3cc3acc07ac7453eab29a2269a2555db","IPY_MODEL_6748471f3a0a4ca19b8860b66a08dabf","IPY_MODEL_5756f121048e449a84b26404aadf80da"],"layout":"IPY_MODEL_a41c9e85cd3d474d9446519d8f1c0bc9"}},"3cc3acc07ac7453eab29a2269a2555db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_846ae1a4116a4d11bfaa4787850313c2","placeholder":"​","style":"IPY_MODEL_5fa947c031364eedb93eab7ebe252808","value":"Downloading (…)okenizer_config.json: 100%"}},"6748471f3a0a4ca19b8860b66a08dabf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d60f14b2e9645739d8cae6cef473128","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a11d1a3a3add41119f5ea479cb36195c","value":28}},"5756f121048e449a84b26404aadf80da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33d8acd3de2f430a8907849c16734608","placeholder":"​","style":"IPY_MODEL_5a52844919244b169eae3a18c7c4e552","value":" 28.0/28.0 [00:00&lt;00:00, 1.64kB/s]"}},"a41c9e85cd3d474d9446519d8f1c0bc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"846ae1a4116a4d11bfaa4787850313c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fa947c031364eedb93eab7ebe252808":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d60f14b2e9645739d8cae6cef473128":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a11d1a3a3add41119f5ea479cb36195c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"33d8acd3de2f430a8907849c16734608":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a52844919244b169eae3a18c7c4e552":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"27620ed028af497a88614776c63b353d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6ee09e9c9ccc4aeb8b178c439ce849cf","IPY_MODEL_eb9a47f38d7e4be18c1545491de83610","IPY_MODEL_9d29b5c03c2a4c5291d6bfab5c97d0c8"],"layout":"IPY_MODEL_b69dd035531f4aaab9845854087c8678"}},"6ee09e9c9ccc4aeb8b178c439ce849cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7caf4a38960c43ac82b16af52ac52cb9","placeholder":"​","style":"IPY_MODEL_c413460ece434641b0650c6685ea318e","value":"Downloading (…)lve/main/config.json: 100%"}},"eb9a47f38d7e4be18c1545491de83610":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_88f68910481c42e7a54d507fd2f68dcb","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_08a49a3d8bce45bc85f1a2cacc747100","value":570}},"9d29b5c03c2a4c5291d6bfab5c97d0c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f6322f768ac4b03ad35e9e52269a900","placeholder":"​","style":"IPY_MODEL_7c678475e1d44f36b84efde15777547d","value":" 570/570 [00:00&lt;00:00, 26.1kB/s]"}},"b69dd035531f4aaab9845854087c8678":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7caf4a38960c43ac82b16af52ac52cb9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c413460ece434641b0650c6685ea318e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88f68910481c42e7a54d507fd2f68dcb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08a49a3d8bce45bc85f1a2cacc747100":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7f6322f768ac4b03ad35e9e52269a900":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c678475e1d44f36b84efde15777547d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ac5bfe6c20940548d23b39cf9afdd75":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0c6a615c35ad43c8b8127c696f0c6fc7","IPY_MODEL_464947777df342f79fec9afdf95705b8","IPY_MODEL_2d58ae7cf36d49fc9c880e4e404dd323"],"layout":"IPY_MODEL_9622339ef64b42d3b15b3493d85d851c"}},"0c6a615c35ad43c8b8127c696f0c6fc7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_00ae08cd7ebb47aa802afe4a134c0e2c","placeholder":"​","style":"IPY_MODEL_f3f94735529a42f29c76362b04272b93","value":"Downloading pytorch_model.bin: 100%"}},"464947777df342f79fec9afdf95705b8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b265a3ab6fe54c54b0f598b33f26c418","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4ac8ab10f672499d98816a4937423ea7","value":440473133}},"2d58ae7cf36d49fc9c880e4e404dd323":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_31241849a28443e6a62957db0c9dfe73","placeholder":"​","style":"IPY_MODEL_59aec8f5422c46a6893e45856a25db6c","value":" 440M/440M [00:06&lt;00:00, 64.1MB/s]"}},"9622339ef64b42d3b15b3493d85d851c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00ae08cd7ebb47aa802afe4a134c0e2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3f94735529a42f29c76362b04272b93":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b265a3ab6fe54c54b0f598b33f26c418":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ac8ab10f672499d98816a4937423ea7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"31241849a28443e6a62957db0c9dfe73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59aec8f5422c46a6893e45856a25db6c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}